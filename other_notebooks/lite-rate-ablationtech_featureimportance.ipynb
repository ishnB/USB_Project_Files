{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa7246a-5be9-4f35-8baa-c5261e9143ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "\n",
    "# # 1. Load your saved fine-tuned model\n",
    "# model_path = \"/home/chills/Desktop/LITE-RATE_files/savedmodels_and_tokenizers/models/albert\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# tokenizer_path = \"/home/chills/Desktop/LITE-RATE_files/savedmodels_and_tokenizers/tokenizers/albert\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "# model.eval()\n",
    "\n",
    "# # 2. Load your CSV with string-formatted features\n",
    "# df = pd.read_csv(\"/home/chills/Desktop/LITE-RATE_files/LITE-RATE-DATAfiles/LITE-RATE-attackdata_80smart20benignmatchingattack_str.csv\")\n",
    "\n",
    "# # 3. Recreate your string representation as you did during training\n",
    "# def create_string_representation(row):\n",
    "#     return \" \".join([f\"{col}: {row[col]}\" for col in df.columns if col != \"Label_code\"])\n",
    "\n",
    "# # Apply the string conversion\n",
    "# df[\"text_representation\"] = df.apply(create_string_representation, axis=1)\n",
    "# texts = df[\"text_representation\"].tolist()\n",
    "\n",
    "# # 4. Set up a prediction function for SHAP\n",
    "# def predict(texts):\n",
    "#     inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     # For binary classification, return probability of class 1 (attack)\n",
    "#     probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "#     return probs[:, 1].numpy()  # Return probability of attack class\n",
    "\n",
    "# # 5. Create a feature importance analysis using feature ablation\n",
    "# feature_names = [\"fw_fl_byt_s\", \"bw_fl_byt_s\", \"fw_fl_pkt_s\", \"bw_fl_pkt_s\", \"fw_pkt_s\", \"bw_pkt_s\"]\n",
    "# baseline_predictions = predict(texts[:100])  # Use a subset for faster processing\n",
    "\n",
    "# # Create a dictionary to store importance scores\n",
    "# feature_importance = {}\n",
    "\n",
    "# # For each feature, measure how predictions change when the feature is removed\n",
    "# for feature in feature_names:\n",
    "#     # Create versions of texts with this feature removed\n",
    "#     modified_texts = []\n",
    "#     for text in texts[:100]:\n",
    "#         # Simple approach: replace the feature and its value with empty string\n",
    "#         modified_text = text.replace(f\"{feature}: \", \"\")\n",
    "#         for token in text.split():\n",
    "#             if token.startswith(f\"{feature}:\"):\n",
    "#                 modified_text = text.replace(token, \"\")\n",
    "#                 break\n",
    "#         modified_texts.append(modified_text)\n",
    "    \n",
    "#     # Get predictions on modified texts\n",
    "#     modified_predictions = predict(modified_texts)\n",
    "    \n",
    "#     # Calculate importance as average absolute difference in predictions\n",
    "#     importance = np.mean(np.abs(baseline_predictions - modified_predictions))\n",
    "#     feature_importance[feature] = importance\n",
    "\n",
    "# # Prepare data for 3D plotting\n",
    "# features = list(feature_importance.keys())\n",
    "# importances = list(feature_importance.values())\n",
    "# indices = np.arange(len(features))  # X-axis: feature indices\n",
    "# heights = importances  # Y-axis: importance values\n",
    "# depths = np.ones(len(features))  # Z-axis: constant depth for visualization\n",
    "\n",
    "# # Create 3D plot\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# # Plot bars in 3D with bright red color\n",
    "# ax.bar3d(indices, depths, np.zeros(len(features)), 0.5, 0.5, heights, color=\"red\", shade=True)\n",
    "\n",
    "# # Customize the plot\n",
    "# ax.set_xlabel(\" \", fontsize=23, labelpad=25)\n",
    "# # ax.set_ylabel(\"Depth\", fontsize=23, labelpad=20)\n",
    "# ax.set_zlabel(\"Importance\", fontsize=23, labelpad=20)\n",
    "# #ax.set_title(\"Feature Importance for Attack Detection\", fontsize=23, pad=10)\n",
    "\n",
    "# # **Adjust view for front-facing perspective**\n",
    "# ax.view_init(elev=0, azim=90)  # Front view (azim=0 aligns depth with Y-axis)\n",
    "\n",
    "# # Set feature names as x-axis ticks **perpendicular to x-axis**\n",
    "# ax.set_xticks(indices)\n",
    "# ax.set_xticklabels(features, rotation=90, ha=\"center\", fontsize=23)  # Rotation=0 for perpendicular orientation\n",
    "\n",
    "# # Set tick font size for all axes\n",
    "# ax.tick_params(axis=\"x\", labelsize=23)\n",
    "# # ax.tick_params(axis=\"y\", labelsize=23)\n",
    "# ax.tick_params(axis=\"z\", labelsize=23)\n",
    "\n",
    "# # Adjust Z-axis tick labels\n",
    "# z_ticks = np.linspace(0, max(heights), num=5)  # Adjust number of ticks as needed\n",
    "# ax.set_zticks(z_ticks)\n",
    "# ax.set_zticklabels([f\"{tick:.2f}\" for tick in z_ticks], fontsize=23)\n",
    "\n",
    "# # Adjust layout\n",
    "# fig.subplots_adjust(left=0.1, right=0.75, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2)\n",
    "\n",
    "# # Save the plot\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"/home/chills/Desktop/LITE-RATE_files/graphs/model_plots_feature_importance/albert/USB_feature_importance_3d_albert.pdf\", dpi=500, bbox_inches=\"tight\")\n",
    "# plt.close()\n",
    "\n",
    "# print(\"3D Feature Importance plot saved as 'USB_feature_importance_3d_albert.pdf'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66a43d7-9ee0-4763-8533-c80f0189a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 15:45:19.760297: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n",
      "2025-03-20 15:45:19.760328: I tensorflow/core/platform/cpu_feature_guard.cc:192] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Feature Importance plot saved as 'USB_feature_importance_3d_tinybert.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "\n",
    "# 1. Load your saved fine-tuned model\n",
    "model_path = \"/home/chills/Desktop/LITE-RATE_files/savedmodels_and_tokenizers/models/tinybert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer_path = \"/home/chills/Desktop/LITE-RATE_files/savedmodels_and_tokenizers/tokenizers/tinybert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model.eval()\n",
    "\n",
    "# 2. Load your CSV with string-formatted features\n",
    "df = pd.read_csv(\"/home/chills/Desktop/LITE-RATE_files/LITE-RATE-DATAfiles/LITE-RATE-attackdata_80smart20benignmatchingattack_str.csv\")\n",
    "\n",
    "# 3. Recreate your string representation as you did during training\n",
    "def create_string_representation(row):\n",
    "    return \" \".join([f\"{col}: {row[col]}\" for col in df.columns if col != \"Label_code\"])\n",
    "\n",
    "# Apply the string conversion\n",
    "df[\"text_representation\"] = df.apply(create_string_representation, axis=1)\n",
    "texts = df[\"text_representation\"].tolist()\n",
    "\n",
    "# 4. Set up a prediction function for SHAP\n",
    "def predict(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # For binary classification, return probability of class 1 (attack)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probs[:, 1].numpy()  # Return probability of attack class\n",
    "\n",
    "# 5. Create a feature importance analysis using feature ablation\n",
    "feature_names = [\"fw_fl_byt_s\", \"bw_fl_byt_s\", \"fw_fl_pkt_s\", \"bw_fl_pkt_s\", \"fw_pkt_s\", \"bw_pkt_s\"]\n",
    "baseline_predictions = predict(texts[:100])  # Use a subset for faster processing\n",
    "\n",
    "# Create a dictionary to store importance scores\n",
    "feature_importance = {}\n",
    "\n",
    "# For each feature, measure how predictions change when the feature is removed\n",
    "for feature in feature_names:\n",
    "    # Create versions of texts with this feature removed\n",
    "    modified_texts = []\n",
    "    for text in texts[:100]:\n",
    "        # Simple approach: replace the feature and its value with empty string\n",
    "        modified_text = text.replace(f\"{feature}: \", \"\")\n",
    "        for token in text.split():\n",
    "            if token.startswith(f\"{feature}:\"):\n",
    "                modified_text = text.replace(token, \"\")\n",
    "                break\n",
    "        modified_texts.append(modified_text)\n",
    "    \n",
    "    # Get predictions on modified texts\n",
    "    modified_predictions = predict(modified_texts)\n",
    "    \n",
    "    # Calculate importance as average absolute difference in predictions\n",
    "    importance = np.mean(np.abs(baseline_predictions - modified_predictions))\n",
    "    feature_importance[feature] = importance\n",
    "\n",
    "# Prepare data for 3D plotting\n",
    "features = list(feature_importance.keys())\n",
    "importances = list(feature_importance.values())\n",
    "indices = np.arange(len(features))  # X-axis: feature indices\n",
    "heights = importances  # Y-axis: importance values\n",
    "depths = np.ones(len(features))  # Z-axis: constant depth for visualization\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot bars in 3D with bright red color\n",
    "ax.bar3d(indices, depths, np.zeros(len(features)), 0.5, 0.5, heights, color=\"blue\", shade=True)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\" \", fontsize=23, labelpad=25)\n",
    "ax.set_zlabel(\"Importance\", fontsize=23, labelpad=35)\n",
    "\n",
    "# **Adjust view for front-facing perspective**\n",
    "ax.view_init(elev=0, azim=90)  # Front view (azim=0 aligns depth with Y-axis)\n",
    "\n",
    "# Set feature names as x-axis ticks **perpendicular to x-axis**\n",
    "ax.set_xticks(indices)\n",
    "ax.set_xticklabels(features, rotation=90, ha=\"center\", fontsize=23)  # Rotation=0 for perpendicular orientation\n",
    "\n",
    "# Remove Y-axis labels and ticks\n",
    "ax.set_yticks([])  # Remove Y-axis ticks\n",
    "ax.set_yticklabels([])  # Remove Y-axis tick labels\n",
    "ax.yaxis.line.set_visible(False)  # Hide Y-axis grid line\n",
    "\n",
    "# Set tick font size for all axes\n",
    "ax.tick_params(axis=\"x\", labelsize=23)\n",
    "ax.tick_params(axis=\"z\", labelsize=23, pad=15)  # Increased padding for visibility\n",
    "\n",
    "# Adjust Z-axis tick labels to avoid overlap with grid\n",
    "z_ticks = np.linspace(0, max(heights), num=5)  # Adjust number of ticks as needed\n",
    "ax.set_zticks(z_ticks)\n",
    "ax.set_zticklabels([f\"{tick:.2f}\" for tick in z_ticks], fontsize=23, verticalalignment='bottom')\n",
    "\n",
    "# Adjust layout\n",
    "fig.subplots_adjust(left=0.1, right=0.75, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2)\n",
    "\n",
    "# Save the plot\n",
    "\n",
    "plt.savefig(\"/home/chills/Desktop/LITE-RATE_files/graphs/model_plots_feature_importance/tinybert/USB_feature_importance_3d_tinybert.pdf\", dpi=500)\n",
    "plt.close()\n",
    "\n",
    "print(\"3D Feature Importance plot saved as 'USB_feature_importance_3d_tinybert.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073bcaec-a367-444e-b0f8-48b1cfde643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional visualization: Calculate importance based on token presence\n",
    "# token_importance = {}\n",
    "\n",
    "# # Process a subset of examples\n",
    "# for text, prediction in zip(texts[:100], baseline_predictions):\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     # For each token, check if removing it affects the prediction\n",
    "#     for token in tokens:\n",
    "#         # Skip tokens that aren't related to features\n",
    "#         if not any(feature in token for feature in feature_names):\n",
    "#             continue\n",
    "        \n",
    "#         # Create a version of text with this token removed\n",
    "#         modified_text = text.replace(token, \"\")\n",
    "#         modified_prediction = predict([modified_text])[0]\n",
    "        \n",
    "#         # Calculate importance as absolute difference in prediction\n",
    "#         importance = abs(prediction - modified_prediction)\n",
    "        \n",
    "#         if token in token_importance:\n",
    "#             token_importance[token].append(importance)\n",
    "#         else:\n",
    "#             token_importance[token] = [importance]\n",
    "\n",
    "# # Calculate average importance for each token\n",
    "# avg_token_importance = {token: np.mean(importances) for token, importances in token_importance.items()}\n",
    "\n",
    "# # Get top tokens by importance\n",
    "# top_tokens = dict(sorted(avg_token_importance.items(), key=lambda x: x[1], reverse=True)[:20])\n",
    "\n",
    "# # Prepare data for 3D plotting\n",
    "# tokens = list(top_tokens.keys())\n",
    "# importances = list(top_tokens.values())\n",
    "# indices = np.arange(len(tokens))  # X-axis: token indices\n",
    "# heights = importances  # Y-axis: importance values\n",
    "# depths = np.ones(len(tokens))  # Z-axis: constant depth for visualization\n",
    "\n",
    "# # Create 3D plot\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Plot bars in 3D with bright red color\n",
    "# ax.bar3d(indices, depths - 3, np.zeros(len(tokens)), 0.4, 0.4, heights, color='red', shade=True)\n",
    "\n",
    "# # Customize the plot\n",
    "# ax.set_xlabel(' ', fontsize=23, labelpad=20)  # Increase label padding and font size\n",
    "# # ax.set_ylabel('Depth', fontsize=23, labelpad=20)       # Increase label padding and font size\n",
    "# ax.set_zlabel('Importance', fontsize=23, labelpad=25)  # Increase label padding and font size\n",
    "# #ax.set_title('Top Tokens by Importance', fontsize=23, pad=10)  # Add padding to title\n",
    "\n",
    "# # Adjust elevation and azimuth angles for better visibility\n",
    "# ax.view_init(elev=0, azim=90)\n",
    "\n",
    "# # Set tick font size for all axes\n",
    "# ax.tick_params(axis='x', labelsize=23)  # Increase X-axis tick font size\n",
    "# # ax.tick_params(axis='y', labelsize=23)  # Increase Y-axis tick font size\n",
    "# ax.tick_params(axis='z', labelsize=23)  # Increase Z-axis tick font size\n",
    "\n",
    "# # Set token names as x-axis ticks with larger font size\n",
    "# ax.set_xticks(indices)\n",
    "# ax.set_xticklabels(tokens, rotation=90, ha='right', fontsize=26)\n",
    "\n",
    "# # Set Z-axis tick labels with increased font size\n",
    "# z_ticks = np.linspace(0, max(heights), num=5)  # Adjust number of ticks as needed\n",
    "# ax.set_zticks(z_ticks)\n",
    "# ax.set_zticklabels([f\"{tick:.2f}\" for tick in z_ticks], fontsize=23)\n",
    "\n",
    "# # Adjust layout\n",
    "# fig.subplots_adjust(left=0.1, right=0.75, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2)\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig(\"/home/chills/Desktop/LITE-RATE_files/graphs/model_plots_feature_importance/albert/top_tokens_importance_3d_albert.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.close()\n",
    "\n",
    "# print(\"3D Top Tokens Importance plot saved as 'top_tokens_importance_3d_albert.pdf'\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2b1b77-b311-417e-8d73-ee38855d7f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Top Tokens Importance plot saved as 'top_tokens_importance_3d_tinybert.pdf'\n"
     ]
    }
   ],
   "source": [
    "# Additional visualization: Calculate importance based on token presence\n",
    "token_importance = {}\n",
    "\n",
    "# Process a subset of examples\n",
    "for text, prediction in zip(texts[:100], baseline_predictions):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # For each token, check if removing it affects the prediction\n",
    "    for token in tokens:\n",
    "        # Skip tokens that aren't related to features\n",
    "        if not any(feature in token for feature in feature_names):\n",
    "            continue\n",
    "        \n",
    "        # Create a version of text with this token removed\n",
    "        modified_text = text.replace(token, \"\")\n",
    "        modified_prediction = predict([modified_text])[0]\n",
    "        \n",
    "        # Calculate importance as absolute difference in prediction\n",
    "        importance = abs(prediction - modified_prediction)\n",
    "        \n",
    "        if token in token_importance:\n",
    "            token_importance[token].append(importance)\n",
    "        else:\n",
    "            token_importance[token] = [importance]\n",
    "\n",
    "# Calculate average importance for each token\n",
    "avg_token_importance = {token: np.mean(importances) for token, importances in token_importance.items()}\n",
    "\n",
    "# Get top tokens by importance\n",
    "top_tokens = dict(sorted(avg_token_importance.items(), key=lambda x: x[1], reverse=True)[:20])\n",
    "\n",
    "# Prepare data for 3D plotting\n",
    "tokens = list(top_tokens.keys())\n",
    "importances = list(top_tokens.values())\n",
    "indices = np.arange(len(tokens))  # X-axis: token indices\n",
    "heights = importances  # Y-axis: importance values\n",
    "depths = np.ones(len(tokens))  # Z-axis: constant depth for visualization\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot bars in 3D with bright red color\n",
    "ax.bar3d(indices, depths - 3, np.zeros(len(tokens)), 0.4, 0.4, heights, color=\"blue\", shade=True)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\" \", fontsize=23, labelpad=20)  # Increase label padding and font size\n",
    "ax.set_zlabel(\"Importance\", fontsize=23, labelpad=35)  # Increase label padding and font size\n",
    "\n",
    "# Adjust elevation and azimuth angles for better visibility\n",
    "ax.view_init(elev=0, azim=90)\n",
    "\n",
    "# Remove Y-axis labels and ticks\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Set tick font size for all axes\n",
    "ax.tick_params(axis=\"x\", labelsize=23)  # Increase X-axis tick font size\n",
    "ax.tick_params(axis=\"z\", labelsize=23, pad=15)  # Increase Z-axis tick font size\n",
    "\n",
    "# Set token names as x-axis ticks with larger font size\n",
    "ax.set_xticks(indices)\n",
    "ax.set_xticklabels(tokens, rotation=90, ha=\"right\", fontsize=26)\n",
    "\n",
    "# Set Z-axis tick labels with increased font size and better spacing\n",
    "z_ticks = np.linspace(0, max(heights), num=5)  # Adjust number of ticks as needed\n",
    "ax.set_zticks(z_ticks)\n",
    "ax.set_zticklabels([f\"{tick:.2f}\" for tick in z_ticks], fontsize=23)\n",
    "\n",
    "# Ensure Z-ticks do not overlap with grid lines\n",
    "for t in ax.get_zticklabels():\n",
    "    t.set_verticalalignment(\"bottom\")\n",
    "\n",
    "# Adjust layout\n",
    "\n",
    "plt.savefig(\n",
    "    \"/home/chills/Desktop/LITE-RATE_files/graphs/model_plots_feature_importance/tinybert/top_tokens_importance_3d_tinybert.pdf\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"3D Top Tokens Importance plot saved as 'top_tokens_importance_3d_tinybert.pdf'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cbb27-4ce9-47c8-84de-6868ff66695f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
